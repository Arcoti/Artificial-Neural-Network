{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2133c166",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "\n",
    "*Note: The code in this document cannot run*\n",
    "\n",
    "## 1. Introduction and Overview\n",
    "\n",
    "### 1.1 Introduction\n",
    "\n",
    "Neural Networks (NN) are like the fundamentals of Deep Learning. With enough data and computational power, they can be used to solve almost any problems. While it is totally fine to treat these NN like black boxes, it's much more exciting and educational to learn what lies behind this term and how they work. \n",
    "\n",
    "As such, this project aims to create an Artificial Neural Network (ANN) from scratch using Python and NumPy. This documentation explains the flow of the ANN and how it works from behind the hood. \n",
    "\n",
    "### 1.2 Overview\n",
    "\n",
    "![Neural Network Diagram](../static/Neural%20Network.png)\n",
    "\n",
    "Above is a very simplistic view of the diagram. It has some coloured circles connected to each other, often referred to as neurons. Basically, how it works it that a Neural Network consists of an Input Layer, multiple Hidden Layers and a final Output Layer. The input is taken in from the Input Layer, passed through multiple Hidden Layers before finally reaching the Output Layer. \n",
    "\n",
    "For this particular ANN, the neurons will train from a set of data, and determine its weights and biases will be determined through backward propagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39aed1b",
   "metadata": {},
   "source": [
    "## 2. Neural Network\n",
    "\n",
    "### 2.1 Initialization of Parameters\n",
    "\n",
    "Referring to the code below, the parameters layer_dims is basically the dimensions of the input, hidden and output layers. For instance, for a 4 layer neural Network with input layer having 784 neurons, the two hiden layers having 128 and 64 neurons respectively and output layer having 10 neurons, the layer_dims will be equal to [784, 128, 64, 10]. In a sense it is taken in as [Input Layer Dimension, Hidden Layer Dimension, ..., Output Layer Dimension].\n",
    "\n",
    "The weights of each neuron are randomly generated via a normal distribution and then multipled by 0.01. \n",
    "The weights are in a $m \\times n$ matrix where m is the current layer dimension and n is the previous layer dimension. In a sense, the weight matrix of the lth layer is as follows:\n",
    "\n",
    "$$\n",
    "weight_{l} = \\begin {bmatrix} w_{1,1} & w_{1,2} & ... & w_{1,n} \\\\ \n",
    "                              w_{2,1} & w_{2,2} & ... & w_{1,n} \\\\\n",
    "                              . \\\\\n",
    "                              . \\\\\n",
    "                              . \\\\\n",
    "                              w_{m,1} & w_{m,2} & ... & w_{m,n}\n",
    "                                      \n",
    "             \\end{bmatrix}\n",
    "$$\n",
    "\n",
    ", where m is the current layer dimension and n is the previous layer dimension.\n",
    "\n",
    "The biases of each neuron are initalized to zero. In a sense the initial bias vector will be basically an $n \\times 1$ column matrix, as such:\n",
    "\n",
    "$$\n",
    "bias_{l} = \\begin {bmatrix} b_{1} \\\\ b_{2} \\\\ . \\\\ . \\\\ . \\\\ b_{m} \\end {bmatrix}\n",
    "$$\n",
    "\n",
    ", where m is the current layer dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b83dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(layer_dims: list[int]):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    parameters = {}\n",
    "\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        # Generation of Weights\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) *0.01\n",
    "        # Generation of Biases\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc36ed",
   "metadata": {},
   "source": [
    "### 2.2 Forward Propagation\n",
    "\n",
    "Forward propagation is basically the process of taking in inputs and making a prediction. \n",
    "\n",
    "#### 2.2.1 Linear Hypothesis\n",
    "\n",
    "For the foward propagation of this particular neural network, and with the assumption that the number of training entries is 1, the Linear Hypothesis is defined as follows: \n",
    "\n",
    "$$\n",
    "Z_{(1, m)} = A_{(1, n)} \\cdot W_{(n, m)} + b_{(1, m)}\n",
    "$$\n",
    "\n",
    ", where $Z_{(1, m)}$ is the $1 \\times m$ linear output, $W_{(n, m)}$ is the $n \\times m$ weight matrix, $A_{(1, n)}$ is the $n \\times 1$ activated output from the previous layer and $b_{(1, m)}$ is the $m \\times 1$ bias vector.\n",
    "\n",
    "#### 2.2.2 Activation Function\n",
    "\n",
    "The activation function for this particular neural network is the sigmoid function. It is defined as follows: \n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "#### 2.2.3 Cache\n",
    "\n",
    "The Linear and Activation Cache are basically the input to the Linear Hypothesis and Activation Function respectively. They are stored and returned for the [backward propagation](#23-backward-propagation) which will be explained in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1dc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X: np.ndarray, params: dict):\n",
    "    A = X\n",
    "    caches = []\n",
    "\n",
    "    for l in range(1, (len(params) // 2) + 1):\n",
    "        A_prev = A\n",
    "        W = params['W'+str(l)]\n",
    "        b = params['b'+str(l)]\n",
    "\n",
    "        # Linear Hypothesis\n",
    "        Z = np.dot(A_prev, W) + b\n",
    "\n",
    "        # Linear Cache\n",
    "        linear_cache = (A_prev, W, b)\n",
    "\n",
    "        # Activation Function\n",
    "        A = sigmoid(Z)                     # type: ignore \n",
    "\n",
    "        # Activation Cache\n",
    "        activation_cache = Z\n",
    "\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39642749",
   "metadata": {},
   "source": [
    "### 2.3 Backward Propagation\n",
    "\n",
    "Backward propagation is slightly more complex than the forward one. In essence, the backward propagation is basically a learning mechanism in the neural network to tune its weights and biases for each neuron. During training, the network evaluates the cost by comparing the prediction to the labels. Then, gradient are determined via backward propagation to make subtle changes to the weights and biases of each neuron. \n",
    "\n",
    "#### 2.3.1 Single Layer Backward Propagation (SLBP)\n",
    "\n",
    "SLBP is basically diving into the backward propgation process of one single layer. Referring to the code below, $dA = \\frac{\\partial cost}{\\partial A}$ where A is its output. In other words, dA is the change in cost with respect to (w.r.t) the change in A. \n",
    "\n",
    "The calculation of $dZ = \\frac{\\partial cost}{\\partial Z}$ where Z is the layer's input is derived from the chain rule. The formula is clearer as follows:\n",
    "\n",
    "$$\n",
    "dZ = \\frac{\\partial cost}{\\partial Z} = \\frac{\\partial cost}{\\partial A} * \\frac{dA}{dZ}\n",
    "$$\n",
    "\n",
    ", where $dA = \\frac{\\partial cost}{\\partial A}$ and $\\frac{dA}{dZ}$ is the first order derivative of the [activation function](#222-activation-function).\n",
    "\n",
    "To obtain dW the formula is shown: \n",
    "\n",
    "$$\n",
    "dW = \\frac{\\partial cost}{\\partial W} = \\frac{1}{m} \\times A_{prev}^{T}  \\cdot \\frac{\\partial cost}{\\partial Z}\n",
    "$$\n",
    "\n",
    ", where m is the total number of training sets, $dZ = \\frac{\\partial cost}{\\partial Z}$ posses the shape of $1 \\times m$ and $A_{prev}$ is the $1 \\times n$ input vector to this particular layer. Do note that dW possess the same shape as the weight matrix and that m is the current layer dimension while n is the previous layer dimension.\n",
    "\n",
    "To obtain db the formula is as follows: \n",
    "\n",
    "$$\n",
    "db = \\frac{\\partial cost}{\\partial b} = \\frac{1}{m} \\times \\begin {bmatrix}\n",
    "\\sum_{i=1}^{m} dZ_{(1, m)} \\\\\n",
    "\\sum_{i=1}^{m} dZ_{(2, m)} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "\\sum_{i=1}^{m} dZ_{(n, m)}\n",
    "\\end {bmatrix}\n",
    "$$\n",
    "\n",
    ", where m is the total number of training sets and n is the current layer dimension, making db a $n \\times 1$ matrix if m = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_layer_back_propagation(dA, cache: tuple):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    Z = activation_cache\n",
    "    dZ = dA * sigmoid_first_derivative(Z) # type: ignore\n",
    "\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(A_prev.T, dZ)\n",
    "    db = (1 / m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "    db = db.reshape(db.shape[1],)\n",
    "    dA_prev = np.dot(dZ, W.T)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf89a4f",
   "metadata": {},
   "source": [
    "#### 2.3.2 Backward Propagation Process\n",
    "\n",
    "After diving deep into the math of the [SLBP](#231-single-layer-backward-propagation-slbp) this is simply applying it across all the layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a89165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, caches):\n",
    "    gradients = {}\n",
    "    L = len(caches)\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    dAL = -(np.divide(Y, AL)) - np.divide(1-Y, 1-AL)\n",
    "\n",
    "    current_cache = caches[L - 1]\n",
    "    gradients['dA'+str(L-1)], gradients['dW'+str(L-1)], gradients['db'+str(L-1)] = one_layer_back_propagation(dAL, current_cache)\n",
    "\n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = one_layer_back_propagation(gradients['dA'+str(l+1)], current_cache)\n",
    "        gradients['dA' + str(l)] = dA_prev_temp\n",
    "        gradients['dW' + str(l + 1)] = dW_temp\n",
    "        gradients['db' + str(l + 1)] = db_temp\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2bdd1",
   "metadata": {},
   "source": [
    "#### 2.3.3 Update of Parameters\n",
    "\n",
    "Referring to the code below, the weights for each layer are simply obtained with the formula as shown:\n",
    "\n",
    "$$\n",
    "W_{new} = W_{current} - rate_{learning} \\times dW\n",
    "$$\n",
    "\n",
    ", where W is the weight matrix and dW is the gradient and both are of the same shape, being $m \\times n$ where m is the dimension of the current layer and n is the dimension of the previous layer. \n",
    "\n",
    "Similarly, biases are obtained by:\n",
    "\n",
    "$$\n",
    "b_{new} = b_{current} - rate_{learning} \\times db\n",
    "$$\n",
    "\n",
    ", where b is the bias matrix and db is the gradient and both are of the same shape, being $m \\times 1$ where m is the dimension of the current layer and 1 assumed number of training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c703020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = parameters['W' + str(l)] - learning_rate * gradients['dW' + str(l)]\n",
    "        parameters['b' + str(l)] = parameters['b' + str(l)] - learning_rate * gradients['db' + str(l)]\n",
    "\n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

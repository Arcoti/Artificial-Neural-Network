{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2133c166",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "\n",
    "## 1. Introduction and Overview\n",
    "\n",
    "### 1.1 Introduction\n",
    "\n",
    "Neural Networks (NN) are like the fundamentals of Deep Learning. With enough data and computational power, they can be used to solve almost any problems. While it is totally fine to treat these NN like black boxes, it's much more exciting and educational to learn what lies behind this term and how they work. \n",
    "\n",
    "As such, this project aims to create an Artificial Neural Network (ANN) from scratch using Python and NumPy. This documentation explains the flow of the ANN and how it works from behind the hood. \n",
    "\n",
    "### 1.2 Overview\n",
    "\n",
    "![Neural Network Diagram](../static/Neural%20Network.png)\n",
    "\n",
    "Above is a very simplistic view of the diagram. It has some coloured circles connected to each other, often referred to as neurons. Basically, how it works it that a Neural Network consists of an Input Layer, multiple Hidden Layers and a final Output Layer. The input is taken in from the Input Layer, passed through multiple Hidden Layers before finally reaching the Output Layer. \n",
    "\n",
    "For this particular ANN, the neurons will train from a set of data, and determine its weights and biases will be determined through backward propagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39aed1b",
   "metadata": {},
   "source": [
    "## 2. Neural Network\n",
    "\n",
    "### 2.1 Initialization of Parameters\n",
    "\n",
    "Referring to the code below, the parameters `layer_dims` is basically the dimensions of the input, hidden and output layers. For instance, for a 4 layer neural Network with input layer having 784 neurons, the two hiden layers having 128 and 64 neurons respectively and output layer having 10 neurons, the `layer_dims` will be equal to `[784, 128, 64, 10]`. In a sense it is taken in as `[Input Layer Dimension, Hidden Layer Dimension, ..., Output Layer Dimension]`.\n",
    "\n",
    "The weights of each neuron are randomly generated via a normal distribution and then multipled by 0.01. \n",
    "The weights are in a $m \\times n$ matrix where m is the current layer dimension and n is the previous layer dimension. In a sense, the weight matrix of the lth layer is as follows:\n",
    "\n",
    "$$\n",
    "weight_{l} = \\begin {bmatrix} w_{1,1} & w_{1,2} & ... & w_{1,n} \\\\ \n",
    "                              w_{2,1} & w_{2,2} & ... & w_{1,n} \\\\\n",
    "                              . \\\\\n",
    "                              . \\\\\n",
    "                              . \\\\\n",
    "                              w_{m,1} & w_{m,2} & ... & w_{m,n}\n",
    "                                      \n",
    "             \\end{bmatrix}\n",
    "$$\n",
    "\n",
    ", where m is the current layer dimension and n is the previous layer dimension.\n",
    "\n",
    "The biases of each neuron are initalized to zero. In a sense the initial bias vector will be basically an $n \\times 1$ column matrix, as such:\n",
    "\n",
    "$$\n",
    "bias_{l} = \\begin {bmatrix} b_{1} \\\\ b_{2} \\\\ . \\\\ . \\\\ . \\\\ b_{m} \\end {bmatrix}\n",
    "$$\n",
    "\n",
    ", where m is the current layer dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b83dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(layer_dims: list[int]):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    parameters = {}\n",
    "\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        # Generation of Weights\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) *0.01\n",
    "        # Generation of Biases\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc36ed",
   "metadata": {},
   "source": [
    "### 2.2 Forward Propagation\n",
    "\n",
    "Forward propagation is basically the process of taking in inputs and making a prediction. \n",
    "\n",
    "For the foward propagation of this particular neural network, and with the assumption that the number of training entries is 1, the Linear Hypothesis is defined as follows: \n",
    "\n",
    "$$\n",
    "Z_{(m, 1)} = W_{(m, n)} \\cdot A_{(n, 1)} + b_{(m, 1)}\n",
    "$$\n",
    "\n",
    ", where $Z_{(m, 1)}$ is the $m \\times 1$ linear output, $W_{(m, n)}$ is the $m \\times n$ weight matrix, $A_{(n, 1)}$ is the $n \\times 1$ activated output from the previous layer and $b_{(m, 1)}$ is the $m \\times 1$ bias vector.\n",
    "\n",
    "The activation function for this particular neural network is the Rectified Linear Unit (ReLU), although more commonly practices uses the sigmoid function. The ReLU is defined as follows: \n",
    "\n",
    "$$\n",
    "ReLU = \\begin {cases} 0 & \\text{if } x < 0 \\\\ x & \\text{if } x \\geq 0 \\end {cases}\n",
    "$$\n",
    "\n",
    "The Linear and Activation Cache are basically the input to the Linear Hypothesis and Activation Function respectively. They are stored and returned for the [backward propagation](#23-backward-propagation) which will be explained in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1dc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X: np.ndarray, params: dict):\n",
    "    A = X\n",
    "    caches = []\n",
    "\n",
    "    for l in range(1, (len(params) // 2) + 1):\n",
    "        A_prev = A\n",
    "        W = params['W'+str(l)]\n",
    "        b = params['b'+str(l)]\n",
    "\n",
    "        # Linear Hypothesis\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "\n",
    "        # Linear Cache\n",
    "        linear_cache = (A_prev, W, b)\n",
    "\n",
    "        # Activation Function\n",
    "        A = ReLU(Z)                     # type: ignore \n",
    "\n",
    "        # Activation Cache\n",
    "        activation_cache = Z\n",
    "\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39642749",
   "metadata": {},
   "source": [
    "### 2.3 Backward Propagation\n",
    "\n",
    "Backward propagation is slightly more complex than the forward one. In essence, the backward propagation is basically a learning mechanism in the neural network to tune its weights and biases for each neuron. During training, the network evaluates the cost by comparing the prediction to the labels. Then, gradient are determined via backward propagation to make subtle changes to the weights and biases of each neuron. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
